{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH04. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.Module` 클래스 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, namedtuple\n",
    "import functools, itertools, weakref, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.utils.hooks as hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _IncompatibleKeys(namedtuple('IncompatibleKeys', ['missing_keys', 'unexpected_key'])):\n",
    "    def __repr__(self):\n",
    "        if not self.missing_keys and not self.unexpected_keys:\n",
    "            return '<All keys matched successfully>'\n",
    "        return super(_IncompatibleKeys, self).__repr__()\n",
    "    \n",
    "    __str__ = __repr__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() missing 2 required positional arguments: 'missing_keys' and 'unexpected_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7407708da5c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __new__() missing 2 required positional arguments: 'missing_keys' and 'unexpected_key'"
     ]
    }
   ],
   "source": [
    "# missing_keys, unexpected_key 두 개의 argument를 필요로 함\n",
    "_IncompatibleKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _addindent(s_, numSpaces):\n",
    "    s = s_.split('\\n')\n",
    "    # don't do anything for single-line stuff\n",
    "    if len(s) == 1:\n",
    "        return s_\n",
    "    first = s.pop(0)\n",
    "    s = [(numSpaces * ' ') + line for line in s]\n",
    "    s = '\\n'.join(s)\n",
    "    s = first + '\\n' + s\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Base class for all neural network modules.\n",
    "    \n",
    "    Module, Buffer, Parameter, Hook >> 이 친구들이 누군지 명확하게 알아야\n",
    "    이 클래스를 정복, 흐름을 장악할 수 있다!\n",
    "    \n",
    "    아직 100% 탐구를 못한 메서드들도 많다...\n",
    "    C++ Engine에서 돌아가는 코드는 지금 못보지만\n",
    "    그게 아닌 메서드들은 어떤 원리로 동작하는지 정확하게 파악하자!\n",
    "    \n",
    "    Usage::\n",
    "        \n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "        \n",
    "        class Model(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Model, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.conv1(x))\n",
    "                return F.relu(self.conv2(x))\n",
    "                \n",
    "    \"\"\"\n",
    "    \n",
    "    dump_patches = False\n",
    "    _version = 1\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "        \"\"\"\n",
    "        torch._C._log_api_usage_once(\"python.nn_module\")  # log 찍는 것?\n",
    "        \n",
    "        self.training = True\n",
    "        self._parameters = OrderedDict()\n",
    "        self._buffers = OrderedDict()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._forward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "        self._state_dict_hooks = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks = OrderedDict()\n",
    "        self._modules = OrderedDict()\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "        Should be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def register_buffer(self, name, tensor):\n",
    "        \"\"\"\n",
    "        Adds a persistent buffer to the module.\n",
    "        \n",
    "        Example::\n",
    "            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        \"\"\"\n",
    "        # 예외 처리\n",
    "        if '_buffers' not in self.__dict__:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign buffer before Module.__init__() call\")\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"buffer name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise keyError(\"buffer name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._buffers:\n",
    "            raise KeyError(\"attributes '{}' already exists\".format(name))\n",
    "            \n",
    "        # tensor 예외 처리\n",
    "        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n",
    "            raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n",
    "                            \"(torch Tensor or None required)\"\n",
    "                            .format(torch.typename(tensor), name))\n",
    "        # 할당\n",
    "        else:\n",
    "            self._buffers[name] = tensor\n",
    "            \n",
    "    def register_parameter(self, name, param):\n",
    "        \"\"\"\n",
    "        Adds a parameter to the module.\n",
    "        \"\"\"\n",
    "        # 예외 처리\n",
    "        if '_parameters' not in self.__dict__:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign parameter before Module.__init__() call\")\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"parameter name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise keyError(\"parameter name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._parameters:\n",
    "            raise KeyError(\"attributes '{}' already exists\".format(name))\n",
    "        \n",
    "        # param 예외 처리\n",
    "        if param is None:\n",
    "            self._parameters[name] = None\n",
    "        elif not isinstance(param, Parameter):\n",
    "            raise TypeError(\"cannot assign '{}' object to parameter '{}' \"\n",
    "                            \"(torch.nn.Parameter or None required)\"\n",
    "                            .format(torch.typename(param), name))\n",
    "        elif params.grad_fn: # 이건 왜 있을까?\n",
    "            raise ValueError(\n",
    "                \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\n",
    "                \"parameters must be created explicitly. To express '{0}' \"\n",
    "                \"as a function of another Tensor, compute the value in \"\n",
    "                \"the forward() method.\".format(name))\n",
    "        else:\n",
    "            self._parametesr[name] = params\n",
    "            \n",
    "    def add_module(self, name, module):\n",
    "        \"\"\"\n",
    "        Adds a child module to the current module.\n",
    "        \"\"\"\n",
    "        # 예외 처리\n",
    "        if not isinstance(module, Module) and module is not None:\n",
    "            raise TypeError(\"{} is not a Module subclass\".format(\n",
    "                torch.typename(module)))\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"module name should be a string. Got {}\".format(\n",
    "                torch.typename(name)))\n",
    "        elif hasattr(self, name) and name not in self._modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        # 할당\n",
    "        self._modules[name] = module\n",
    "        \n",
    "    def _apply(self, fn):\n",
    "        for module in self.children():\n",
    "            module._apply(fn)\n",
    "        \n",
    "        def compute_should_use_set_data(tensor, tensor_applied):\n",
    "            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
    "                \"\"\"\n",
    "                Defined in File Functions.h\n",
    "                \n",
    "                If the new tensor has compatible tensor type as the existing tensor,\n",
    "                the current behavior is to change the tensor in-place using `.data =`,\n",
    "                and the future behavior is to overwrite the existing tensor. However,\n",
    "                changing the current behavior is a BC-breaking change, and we want it\n",
    "                to happen in future releases. So for now we introduce the\n",
    "                `torch.__future__.get_overwrite_module_params_on_conversion()`\n",
    "                global flag to let the user control whether they want the future\n",
    "                behavior of overwriting the existing tensor or not.\n",
    "                \"\"\"\n",
    "                return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "        for key, param in self._parameters.items():\n",
    "            if param is not None:\n",
    "                with torch.no_grad():\n",
    "                    param_applied = fn(param)\n",
    "                should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
    "                if should_use_set_data:\n",
    "                    param.data = param_applied\n",
    "                else:\n",
    "                    assert isinstance(param, Parameter)\n",
    "                    assert param.is_leaf\n",
    "                    self._parameters[key] = Parameter(param_applied, param.requres_grad)\n",
    "                    \n",
    "                if paaram.grad is not None:\n",
    "                    with torch.no_grad():\n",
    "                        grad_applied = fn(param.grad)\n",
    "                    should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)\n",
    "                    if should_ues_set_data:\n",
    "                        param.grad = grad_applied\n",
    "                    else:\n",
    "                        assert param.grad.is_leaf\n",
    "                        self._parameters[key].grad = grad_applied.requires_grad_(param.grad.requires_grad)\n",
    "        \n",
    "        for key, buf in self._buffers.items():\n",
    "            if buf is not None:\n",
    "                self._buffers[key] = fn(buf)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def apply(self, fn):\n",
    "        \"\"\"\n",
    "        Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
    "        \n",
    "        Example::\n",
    "            >>> @torch.no_grad()\n",
    "            >>> def init_weights(m):\n",
    "            >>>     print(m)\n",
    "            >>>     if type(m) == nn.Linear:\n",
    "            >>>         m.weight.fill_(1.0)\n",
    "            >>>         print(m.weight)\n",
    "            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "            >>> net.apply(init_weights)\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            Parameter containing:\n",
    "            tensor([[ 1.,  1.],\n",
    "                    [ 1.,  1.]])\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            Parameter containing:\n",
    "            tensor([[ 1.,  1.],\n",
    "                    [ 1.,  1.]])\n",
    "            Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            )\n",
    "            Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            )\n",
    "        \"\"\"\n",
    "        for module in self.children():\n",
    "            module.apply(fn)\n",
    "        fn(self)\n",
    "        return self\n",
    "        \n",
    "    def cuda(self, device=None):\n",
    "        \"\"\"\n",
    "        Moves all model parameters and buffers to the GPU.\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "    \n",
    "    def cpu(self):\n",
    "        \"\"\"\n",
    "        Moves all model parameters and buffers to the CPU.\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.cpu())\n",
    "    \n",
    "    def type(self, dst_type):\n",
    "        \"\"\"\n",
    "        Casts all parameters and buffers to :attr:`dst_type`.\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.type(dst_type))\n",
    "    \n",
    "    def float(self):\n",
    "        \"\"\"\n",
    "        Casts all floating point paramters and buffers to ``float`` datatype.\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.float() if t.is_floating_point() else t)\n",
    "    \n",
    "    def double(self):\n",
    "        \"\"\"\n",
    "        Casts all floating point parameters and buffers to ``double`` datatype.\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.double() if t.is_floating_point() else t)\n",
    "    \n",
    "    def half(self):\n",
    "        \"\"\"\n",
    "        Casts all floating point parameters and buffers to ``half`` datatype.\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.half() if t.is_floating_point() else t)\n",
    "    \n",
    "    def bfloat16(self):\n",
    "        \"\"\"\n",
    "        Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.bfloat16() if t.is_floating_point() else t)\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Moves and/or casts the parameters and buffers.\n",
    "        \n",
    "        This can be called as\n",
    "        .. function:: to(device=None, dtype=None, non_blocking=False)\n",
    "        .. function:: to(dtype, non_blocking=False)\n",
    "        .. function:: to(tensor, non_blocking=False)\n",
    "        .. function:: to(memory_format=torch.channels_last)\n",
    "        \n",
    "        Example::\n",
    "            >>> linear = nn.Linear(2, 2)\n",
    "            >>> linear.weight\n",
    "            Parameter containing:\n",
    "            tensor([[ 0.1913, -0.3420],\n",
    "                    [-0.5113, -0.2325]])\n",
    "            >>> linear.to(torch.double)\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            >>> linear.weight\n",
    "            Parameter containing:\n",
    "            tensor([[ 0.1913, -0.3420],\n",
    "                    [-0.5113, -0.2325]], dtype=torch.float64)\n",
    "            >>> gpu1 = torch.device(\"cuda:1\")\n",
    "            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            >>> linear.weight\n",
    "            Parameter containing:\n",
    "            tensor([[ 0.1914, -0.3420],\n",
    "                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
    "            >>> cpu = torch.device(\"cpu\")\n",
    "            >>> linear.to(cpu)\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            >>> linear.weight\n",
    "            Parameter containing:\n",
    "            tensor([[ 0.1914, -0.3420],\n",
    "                    [-0.5112, -0.2324]], dtype=torch.float16)\n",
    "        \"\"\"\n",
    "        \n",
    "        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
    "        \n",
    "        if dtype is not None:\n",
    "            if not dtype.is_floating_point:\n",
    "                raise TypeError('nn.Module.to only accepts floating point '\n",
    "                                'dtypes, but got desired dtype={}',format(dtype))\n",
    "                \n",
    "        def convert(t):\n",
    "            if convert_to_format is not None and t.dim() == 4:\n",
    "                return t.to(device, dtype if t.is_floating_point() else None, non_blocking, \n",
    "                            memory_format=convert_to_format)\n",
    "            return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
    "        \n",
    "        return self._apply(convert)\n",
    "    \n",
    "    \"\"\"\n",
    "    Hook 추가 함수\n",
    "        - self.register_backward_hook\n",
    "        - self.register_forward_pre_hook\n",
    "        - self.register_forward_hook\n",
    "        \n",
    "    What is Hook?\n",
    "        Hook: 계층의 출력이나 grad_output 을 살펴보거나 수정\n",
    "    \"\"\"\n",
    "    def register_backward_hook(self, hook):\n",
    "        \"\"\"\n",
    "        Registers a backward hook on the module.\n",
    "        \"\"\"\n",
    "        handle = hooks.RemovableHandle(self._backward_hooks)\n",
    "        self._backward_hooks[handle.id] = hook\n",
    "        return handle\n",
    "    \n",
    "    def register_forward_pre_hook(self, hook):\n",
    "        \"\"\"\n",
    "        Registers a forward pre-hook on the module.\n",
    "        \"\"\"\n",
    "        handle = hooks.RemovableHandle(self._forward_pre_hooks)\n",
    "        self._forward_pre_hooks[handle.id] = hook\n",
    "        return handle\n",
    "    \n",
    "    def register_forward_hook(self, hook):\n",
    "        \"\"\"\n",
    "        Registers a forward hook on the module.\n",
    "        \"\"\"\n",
    "        handle = hooks.RemovableHandle(self._forward_hooks)\n",
    "        self._forward_hooks[handle.id] = hook\n",
    "        return handle\n",
    "    \n",
    "    \"\"\"이건 아직도 모르겠어! 나중에 살펴보자.\"\"\"\n",
    "    def _slow_forward(self, *input, **kwargs):\n",
    "        tracing_state = torch._C._get_tracing_state()\n",
    "        if not tracing_state or isinstance(self.forward, torch._C.ScriptMethod):\n",
    "            return self.forward(*input, **kwargs)\n",
    "        recording_scopes = torch.jit._trace_module_map is not None\n",
    "        if recording_scopes:\n",
    "            name = torch.jit._trace_module_map[self] if self in torch.jit._trace_module_map else None\n",
    "            if name:\n",
    "                cur_scope_name = tracing_state.current_scope()\n",
    "                tracing_state.push_scope(name)\n",
    "            else:\n",
    "                recording_scopes = False\n",
    "        try:\n",
    "            result = self.forward(*input, **kwargs)\n",
    "        finally:\n",
    "            if recording_scopes:\n",
    "                tracing_state.pop_scope()\n",
    "        return result\n",
    "    \n",
    "    def __call__(self, *input, **kwargs):\n",
    "        \"\"\"\n",
    "        < Callable Object >\n",
    "        Instance가 호출됐을 때 실행\n",
    "        \n",
    "        `x()`와 `x.__call__()`이 동일!\n",
    "        \"\"\"\n",
    "        for hook in self._forward_pre_hooks.values():\n",
    "            result = hook(self, input)\n",
    "            if result is not None:\n",
    "                if not isinstance(result, tuple):\n",
    "                    result = (result,)\n",
    "                input = result\n",
    "        if torch._C._get_tracing_state():\n",
    "            result = self._slow_forward(*input, **kwargs)\n",
    "        else:\n",
    "            result = self.forward(*input, **kwargs)\n",
    "        for hook in self._forward_hooks.values():\n",
    "            hook_result = hook(self, input, result)\n",
    "            if hook_result is not None:\n",
    "                result = hook_result\n",
    "        if len(self._backward_hooks) > 0:\n",
    "            var = result\n",
    "            while not isinstance(var, torch.Tensor):\n",
    "                if isinstance(var, dict):\n",
    "                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
    "                else:\n",
    "                    var = var[0]\n",
    "            grad_fn = var.grad_fn\n",
    "            if grad_fn is not None:\n",
    "                for hook in self._backward_hooks.values():\n",
    "                    wrapper = functools.partial(hook, self)\n",
    "                    functools.update_wrapper(wrapper, hook)\n",
    "                    grad_fn.register_hook(wrapper)\n",
    "        return result\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"\n",
    "        < Object Pickling >\n",
    "        what is pickling?\n",
    "            파이썬 데이터 구조의 직렬화 프로세스\n",
    "            객체를 저장하고 나중에 검색(캐싱)할 때 매우 유용\n",
    "            걱정과 혼란의 주 요인\n",
    "            \n",
    "        `__setstate__(self, state)`: 객체가 unpickle되었을 때\n",
    "            객체의 상태는 객체의 `__dict__`에 직접 적용되지 않고 전달.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state)\n",
    "        # Support loading old checkpints that don't have the following attrs:\n",
    "        if '_forward_pre_hooks' not in self.__dict__:\n",
    "            self._forward_pre_hooks = OrderedDict()\n",
    "        if '_state_dict_hooks' not in self.__dict__:\n",
    "            self._state_dict_hooks = OrderedDict()\n",
    "        if '_load_state_dict_pre_hooks' not in self.__dict__:\n",
    "            self._load_state_dict_pre_hooks = OrderedDict()\n",
    "            \n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        < 속성 접근 제어하기 >\n",
    "        파이썬은 클래스에 대한 진정한 캡슐화가 부족한가?\n",
    "        getter, setter를 사용하여 개인 속성을 정의할 수 있는 방법이 없는가?\n",
    "        Nope!\n",
    "        \"매직\"을 통해 많은 양의 캡슐화를 그냥 수행\n",
    "        \n",
    "        `__getattr__(self, name)`: 사용자가 존재하지 않는 속성에 \n",
    "            엑세스하려고 시도할 때의 행위를 정의.\n",
    "            일반적인 맞춤법 오류를 포착, 리다이렉트,\n",
    "            더 이상 사용되지 않는 속성 \n",
    "            (원하는 경우 해당 속성을 계산하고 반환하도록 선택 가능)\n",
    "            사용에 대한 경고를 제공하거나,\n",
    "            `AttributeError`를 손쉽게 전달할 때 유용.\n",
    "            존재하지 않는 속성에 엑세스할 때만 호출되므로 실제 캡슐화 솔루션 X\n",
    "        \"\"\"\n",
    "        if '_parameters' in self.__dict__:\n",
    "            _parameters = self.__dict__['_parameters']\n",
    "            if name in _parameters:\n",
    "                return _parameters[name]\n",
    "        if '_buffers' in self.__dict__:\n",
    "            _buffers = self.__dict__['_buffers']\n",
    "            if name in _buffers:\n",
    "                return _buffers[name]\n",
    "        if '_modules' in self.__dict__:\n",
    "            modules = self.__dict__['_modules']\n",
    "            if name in modules:\n",
    "                return modules[name]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, name))\n",
    "            \n",
    "    def __setattr__(self, name, value):\n",
    "        \"\"\"\n",
    "        < 속성 접근 제어하기 >\n",
    "        파이썬은 클래스에 대한 진정한 캡슐화가 부족한가?\n",
    "        getter, setter를 사용하여 개인 속성을 정의할 수 있는 방법이 없는가?\n",
    "        Nope!\n",
    "        \"매직\"을 통해 많은 양의 캡슐화를 그냥 수행\n",
    "        \n",
    "        `__setattr__(self, name, value)`: 캡슐화 솔루션\n",
    "            특성값의 변경 사항에 대한 사용자 지정 규칙을 정의\n",
    "            해당 특성의 존재 여부에 관계없이 특성에 할당할 동작 정의\n",
    "            \n",
    "        Caution!!\n",
    "        ```python\n",
    "        def __setattr__(self, name, value):\n",
    "            self.name = value\n",
    "            # 속성이 할당될 때마다 __setattr__()이 호출. (재귀)\n",
    "            # 이는 self.__setattr__(name, value)를 의미\n",
    "            # 무한 재귀 발생, 이를 방지해줘야 함\n",
    "            \n",
    "        def __setattr__(self, name, value):\n",
    "            self.__dict__[name] = value # 클래스의 dict의 이름에 할당\n",
    "            # 커스톰 동작을 정의\n",
    "        ```\n",
    "        \"\"\"\n",
    "        def remove_from(*dict):\n",
    "            for d in dicts:\n",
    "                if name in d:\n",
    "                    del d[name]\n",
    "                    \n",
    "        params = self.__dict__.get('_paramters')\n",
    "        if isinstance(value, Parameter):\n",
    "            if params is None:\n",
    "                raise AttributeError(\n",
    "                    \"cannot assign parameters before Module.__init__() call\")\n",
    "            remove_from(self.__dict__, self._buffers, self._modules)\n",
    "            self.register_parameter(name, value)\n",
    "        elif params is not None and name in params:\n",
    "            if value is not None:\n",
    "                raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n",
    "                                \"(torch.nn.Parameter or None expected)\"\n",
    "                                .format(torch.typename(value), name))\n",
    "            self.register_parameter(name, value)\n",
    "        else:\n",
    "            modules = self.__dict__.get('_modules')\n",
    "            if isinstance(value, Module):\n",
    "                if modules is None:\n",
    "                    raise AttributeError(\n",
    "                        \"cannot assign module before Module.__init__() call\")\n",
    "                remove_from(self.__dict__, self._parameters, self._buffers)\n",
    "                modules[name] = value\n",
    "            elif modules is not None and name in modules:\n",
    "                if value is not None:\n",
    "                    raise ValueError(\"cannot assign '{}' as child module '{}' \"\n",
    "                                     \"(torch.nn.Module or None expected)\"\n",
    "                                     .format(torch.typename(value), name))\n",
    "            else:\n",
    "                buffers = self.__dict__.get('_buffers')\n",
    "                if buffers is not None and name in buffers:\n",
    "                    if value is not None and not isinstance(value, torch.Tensor):\n",
    "                        raise TypeError(\"cannot assign '{}' as buffer '{}' \"\n",
    "                                        \"(torch.Tensor or None expected)\"\n",
    "                                        .format(torch.typename(value), name))\n",
    "                    buffers[name] = value\n",
    "                else:\n",
    "                    object.__setattr__(self, name, value)\n",
    "                    \n",
    "    def __delattr__(self, name):\n",
    "        \"\"\"\n",
    "        < 속성 접근 제어하기 >\n",
    "        파이썬은 클래스에 대한 진정한 캡슐화가 부족한가?\n",
    "        getter, setter를 사용하여 개인 속성을 정의할 수 있는 방법이 없는가?\n",
    "        Nope!\n",
    "        \"매직\"을 통해 많은 양의 캡슐화를 그냥 수행\n",
    "        \n",
    "        `__delattr__(self, name)`: `__setattr__`과 완전히 동일\n",
    "            그러나 속성을 설정하는 대신 삭제하는 것.\n",
    "            무한 재귀(`__delattr__` 구현시 `del self.name`을 호출하면\n",
    "            무한 재귀가 발생)를 방지하기 위해 `__setattr__`과 동일한 예방 조치를\n",
    "            취해야한다.\n",
    "        \"\"\"\n",
    "        if name in self._parameters:\n",
    "            del self._parameters[name]\n",
    "        elif name in self._buffers:\n",
    "            del self._buffers[name]\n",
    "        elif name in self._modules:\n",
    "            del self._modules[name]\n",
    "        else:\n",
    "            object.__delattr__(self, name) # self말고 object로 호출\n",
    "            \n",
    "    \"\"\"\n",
    "    state_dict_hook 관련 함수들\n",
    "    \n",
    "    뭘 위해서 존재하는지를 공부하자!\n",
    "    \"\"\"\n",
    "    def _register_state_dict_hook(self, hook):\n",
    "        handle = hooks.RemovableHandle(self._state_dict_hooks)\n",
    "        self._state_dict_hooks[handle.id] = hook\n",
    "        return handle\n",
    "    \n",
    "    def _save_to_state_dict(self, destination, prefix, keep_vars):\n",
    "        \"\"\"\n",
    "        Saves module state to `destination` dictionary, containing a state\n",
    "        of the module, but not its descendants. This is called on every\n",
    "        submodule in :meth:`~torch.nn.Module.state_dict`.\n",
    "        \"\"\"\n",
    "        for name, param in self._parameters.items():\n",
    "            if param is not None:\n",
    "                destination[prefix + name] = param if keep_vars else param.detach()\n",
    "        for name, buf in self._buffers.items():\n",
    "            if buf is not None:\n",
    "                desetination[prefix + name] = buf if keep_vars else buf.detach()\n",
    "                \n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing a whole state of the module.\n",
    "        \n",
    "        Example::\n",
    "            >>> module.state_dict().keys()\n",
    "            ['bias', 'weight']\n",
    "        \"\"\"\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = local_metadata = dict(version=self._version)\n",
    "        self._save_to_state_dict(destination, prefix, keep_vars)\n",
    "        for name, module in self._modules.items():\n",
    "            if module is not None:\n",
    "                module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)\n",
    "        for hook in self._state_dict_hooks.values():\n",
    "            hook_result = hook(self, destination, prefix, local_metadata)\n",
    "            if hook_result is not None:\n",
    "                destination = hook_result\n",
    "        return destination\n",
    "    \n",
    "    def _register_load_state_dict_pre_hook(self, hook):\n",
    "        handle = hooks.RemovableHandle(self._load_state_dict_pre_hooks)\n",
    "        self._load_state_dict_pre_hooks[handle.id] = hook\n",
    "        return handle\n",
    "    \n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        \"\"\"\n",
    "        Copies parameters and buffers from :attr:`state_dict` into only\n",
    "        this module, but not its descendants.\n",
    "        \"\"\"\n",
    "        for hook in self._load_state_dict_pre_hooks.values():\n",
    "            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "        local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())\n",
    "        local_state = {k: v for k, v in local_name_params if v is not None}\n",
    "\n",
    "        for name, param in local_state.items():\n",
    "            key = prefix + name\n",
    "            if key in state_dict:\n",
    "                input_param = state_dict[key]\n",
    "\n",
    "                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "                if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                    input_param = input_param[0]\n",
    "\n",
    "                if input_param.shape != param.shape:\n",
    "                    # local shape should match the one in checkpoint\n",
    "                    error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                                      'the shape in current model is {}.'\n",
    "                                      .format(key, input_param.shape, param.shape))\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        param.copy_(input_param)\n",
    "                except Exception as ex:\n",
    "                    error_msgs.append('While copying the parameter named \"{}\", '\n",
    "                                      'whose dimensions in the model are {} and '\n",
    "                                      'whose dimensions in the checkpoint are {}, '\n",
    "                                      'an exception occured : {}.'\n",
    "                                      .format(key, param.size(), input_param.size(), ex.args))\n",
    "            elif strict:\n",
    "                missing_keys.append(key)\n",
    "\n",
    "        if strict:\n",
    "            for key in state_dict.keys():\n",
    "                if key.startswith(prefix):\n",
    "                    input_name = key[len(prefix):]\n",
    "                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n",
    "                    if input_name not in self._modules and input_name not in local_state:\n",
    "                        unexpected_keys.append(key)\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        \"\"\"\n",
    "        Copies parameters and buffers from :attr:`state_dict` into\n",
    "        this module and its descendants.\n",
    "        \"\"\"\n",
    "        missing_keys = []\n",
    "        unexpected_keys = []\n",
    "        error_msgs = []\n",
    "\n",
    "        # copy state_dict so _load_from_state_dict can modify it\n",
    "        metadata = getattr(state_dict, '_metadata', None)\n",
    "        state_dict = state_dict.copy()\n",
    "        if metadata is not None:\n",
    "            state_dict._metadata = metadata\n",
    "\n",
    "        def load(module, prefix=''):\n",
    "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "            module._load_from_state_dict(\n",
    "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + '.')\n",
    "\n",
    "        load(self)\n",
    "        load = None  # break load->load reference cycle\n",
    "\n",
    "        if strict:\n",
    "            if len(unexpected_keys) > 0:\n",
    "                error_msgs.insert(\n",
    "                    0, 'Unexpected key(s) in state_dict: {}. '.format(\n",
    "                        ', '.join('\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "            if len(missing_keys) > 0:\n",
    "                error_msgs.insert(\n",
    "                    0, 'Missing key(s) in state_dict: {}. '.format(\n",
    "                        ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "        if len(error_msgs) > 0:\n",
    "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                               self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "        return _IncompatibleKeys(missing_keys, unexpected_keys) # 여기서 나오는구나!\n",
    "    \n",
    "    def _named_members(self, get_members_fn, prefix='', recurse=True):\n",
    "        \"\"\"\n",
    "        Helper method for yielding various names + members of modules.\n",
    "        \"\"\"\n",
    "        memo = set()\n",
    "        modules = self.named_modules(prefix=prefix) if recurse else [(prefix, self)]\n",
    "        for module_prefix, module in modules:\n",
    "            members = get_members_fn(module)\n",
    "            for k, v in members:\n",
    "                if v is None or v in memo:\n",
    "                    continue\n",
    "                memo.add(v)\n",
    "                name = module_prefix + ('.' if module_prefix else '') + k\n",
    "                yield name, v\n",
    "                \n",
    "    def parameters(self, recurse=True):\n",
    "        \"\"\"\n",
    "        Returns an iterator over module parameters.\n",
    "        \n",
    "        Example::\n",
    "            >>> for param in model.parameters():\n",
    "            >>>     print(type(param), param.size())\n",
    "            <class 'torch.Tensor'> (20L,)\n",
    "            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters(recurse=recurse):\n",
    "            yield param\n",
    "            \n",
    "    def named_parameters(self, prefix='', recurse=True):\n",
    "        \"\"\"\n",
    "        Returns an iterator over module parameters, yielding both the\n",
    "        name of the parameter as well as the parameter itself.\n",
    "        \n",
    "        Example::\n",
    "            >>> for name, param in self.named_parameters():\n",
    "            >>>     if name in ['bias']:\n",
    "            >>>         print(param.size())\n",
    "        \"\"\"\n",
    "        gen = self._named_members(\n",
    "            lambda module: module._parameters.items(),\n",
    "            prefix=prefix, recurse=recurse)\n",
    "        for elem in gen:\n",
    "            yield elem\n",
    "            \n",
    "    def buffers(self, recurse=True):\n",
    "        \"\"\"\n",
    "        Returns an iterator over module buffers.\n",
    "        \n",
    "        Example::\n",
    "            >>> for buf in model.buffers():\n",
    "            >>>     print(type(buf), buf.size())\n",
    "            <class 'torch.Tensor'> (20L,)\n",
    "            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
    "        \"\"\"\n",
    "        for name, buf in self.named_buffers(recurse=recurse):\n",
    "            yield buf\n",
    "            \n",
    "    def named_buffers(self, prefix='', recurse=True):\n",
    "        \"\"\"\n",
    "        Returns an iterator over module buffers, yielding both the\n",
    "        name of the buffer as well as the buffer itself.\n",
    "        \n",
    "        Example::\n",
    "            >>> for name, buf in self.named_buffers():\n",
    "            >>>     if name in ['running_var']:\n",
    "            >>>         print(buf.size())\n",
    "        \"\"\"\n",
    "        gen = self._named_members(\n",
    "            lambda module: module._buffers.items(),\n",
    "            prefix=prefix, recurse=recurse)\n",
    "        for elem in gen:\n",
    "            yield elem\n",
    "\n",
    "    def children(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over immediate children modules.\n",
    "        \n",
    "        Yields:\n",
    "            Module: a child module\n",
    "        \"\"\"\n",
    "        for name, module in self.named_children():\n",
    "            yield module\n",
    "            \n",
    "    def named_children(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over immediate children modules, yielding both\n",
    "        the name of the module as well as the module itself.\n",
    "        \n",
    "        Yields:\n",
    "            (string, Module): Tuple containing a name and child module\n",
    "            \n",
    "        Example:\n",
    "            >>> for name, module in model.named_children():\n",
    "            >>>     if name in ['conv4', 'conv5']:\n",
    "            >>>         print(module)\n",
    "        \"\"\"\n",
    "        memo = set()\n",
    "        for name, module in self._modules.items():\n",
    "            if module is not None and module not in memo:\n",
    "                memo.add(module)\n",
    "                yield name, module\n",
    "                \n",
    "    def modules(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over all members in the network.\n",
    "        \n",
    "        Yields:\n",
    "            Module: a module in the network\n",
    "            \n",
    "        Note:\n",
    "            Duplicate modules are returned only once. In the following\n",
    "            example, ``l`` will be returned only once.\n",
    "            \n",
    "        Example::\n",
    "            >>> l = nn.Linear(2, 2)\n",
    "            >>> net = nn.Sequential(1, 1)\n",
    "            >>> for idx, m in enumerate(net.modules()):\n",
    "            >>>     print(idx, '->', m)\n",
    "            \n",
    "            0 -> Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            )\n",
    "            1 -> Linear(in_features=2, out_features=2, bias=True)\n",
    "        \"\"\"\n",
    "        for name, module in self.named_modules():\n",
    "            yield module\n",
    "            \n",
    "    def named_modules(self, memo=None, prefix=''):\n",
    "        \"\"\"\n",
    "        Returns an iterator over all modules in the network, yielding\n",
    "        both the name of the module as well as the module itself.\n",
    "        \n",
    "        Yield:\n",
    "            (string, Module): Tuple of name and module\n",
    "            \n",
    "        Note:\n",
    "            Duplicate modules are returned only once. In the following\n",
    "            example, ``l`` will be returned only once.\n",
    "            \n",
    "        Example::\n",
    "            >>> l = nn.Linear(2, 2)\n",
    "            >>> net = nn.Sequential(l, l)\n",
    "            >>> for idx, m in enumerate(net.named_modules()):\n",
    "                    print(idx, '->', m)\n",
    "            0 -> ('', Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            ))\n",
    "            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
    "        \"\"\"\n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "        if self not in memo:\n",
    "            memo.add(self)\n",
    "            yield prefix, self\n",
    "            for name, module in self._modules.items():\n",
    "                if module is None:\n",
    "                    continue\n",
    "                submodule_prefix = prefix + ('.' if prefix else '') + name\n",
    "                for m in module.named_modules(memo, submodule_prefix):\n",
    "                    yield m\n",
    "                    \n",
    "    def train(self, mode=True):\n",
    "        \"\"\"\n",
    "        Sets the module in training mode.\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "        for module in self.children():\n",
    "            module.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def eval(self, mode=True):\n",
    "        \"\"\"\n",
    "        Sets the module in evaluation mode.\n",
    "        \"\"\"\n",
    "        return self.train(False)\n",
    "    \n",
    "    def requires_grad_(self, requires_grad=True):\n",
    "        \"\"\"\n",
    "        Change if autograd should record operation on parameters in this\n",
    "        module.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_(requires_grad)\n",
    "        return self\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets gradients of all model parameters to zero.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n",
    "                \n",
    "    def share_memory(self):\n",
    "        return self._apply(lambda t: t.share_memory_())\n",
    "    \n",
    "    def _get_name(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        \"\"\"\n",
    "        Set the extra representation of the module\n",
    "        \"\"\"\n",
    "        return ''\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        < 클래스 표현하기 >\n",
    "        클래스를 문자열로 표현!\n",
    "        \n",
    "        `__repr__(self)`: 클래스의 인스턴스에서 `repr()`이 호출될 때의\n",
    "            동작을 정의. `repr()`은 주로 기계가 읽을 수 있는 출력을 대상으로,\n",
    "            `str()`은 사람이 읽을 수 있도록 만들어짐.\n",
    "        \"\"\"\n",
    "        # We treat the extra repr like the sub-module, one item per line\n",
    "        extra_lines = []\n",
    "        extra_repr = self.extra_repr()\n",
    "        # empty string will be split into list ['']\n",
    "        if extra_repr:\n",
    "            extra_lines = extra_repr.split('\\n')\n",
    "        child_lines = []\n",
    "        for key, module in self._modules.items():\n",
    "            mod_str = repr(module)\n",
    "            mod_str = _addindent(mod_str, 2)\n",
    "            child_lines.append('(' + key + '): ' + mod_str)\n",
    "        lines = extra_lines + child_lines\n",
    "\n",
    "        main_str = self._get_name() + '('\n",
    "        if lines:\n",
    "            # simple one-liner info, which most builtin Modules will use\n",
    "            if len(extra_lines) == 1 and not child_lines:\n",
    "                main_str += extra_lines[0]\n",
    "            else:\n",
    "                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "\n",
    "        main_str += ')'\n",
    "        return main_str\n",
    "    \n",
    "    def __dir__(self):\n",
    "        \"\"\"\n",
    "        < 클래스 표현하기 >\n",
    "        클래스를 문자열로 표현!\n",
    "        \n",
    "        `__dir__(self)`: 클래스의 인스턴스에서 `dir()`이 호출될 때의\n",
    "            동작을 정의. 이 메서드는 사용자의 attribute 목록을 반환.\n",
    "            일반적으로 `__dir__`을 구현하는 것은 불필요, `__getattr__` 또는\n",
    "            `__getattribute__`를 재정의하거나 그렇지 않으면 동적으로\n",
    "            속성을 생성하는 경우 클래스를 대화식으로 사용하는 것이\n",
    "            매우 중요할 수 있음.\n",
    "        \"\"\"\n",
    "        module_attrs = dir(self.__class__)\n",
    "        attrs = list(self.__dict__.keys())\n",
    "        parameters = list(self._parameters.keys())\n",
    "        modules = list(self._modules.keys())\n",
    "        buffers = list(self._buffers.keys())\n",
    "        keys = module_attrs + attrs + parameters + modules + buffers\n",
    "\n",
    "        # Eliminate attrs that are not legal Python variable names\n",
    "        keys = [key for key in keys if not key[0].isdigit()]\n",
    "\n",
    "        return sorted(keys)\n",
    "    \n",
    "    def _replicate_for_data_parallel(self):\n",
    "        replica = self.__new__(type(self))\n",
    "        \"\"\"\n",
    "        < 생성 및 초기화 >\n",
    "        가장 기본적인 매직 메서드인 `__init__`는 모두 알고 있음.\n",
    "        그러나 `x = SomeClass()`를 호출하면 `__init__`이 먼저 호출되지 않음.\n",
    "        사실 `__new__` 메서드가 먼저 실행되고 실제로 인스턴스를 생성한 다음\n",
    "        생성시에 인수를 초기화 프로그램에 전달.\n",
    "\n",
    "        `__new__(cls, [...)`: 객체의 인스턴스화에서 호출되는 첫 번째 메서드.\n",
    "            클래스를 취한 다음 `__init__`에 전달할 다른 인수를 취함.\n",
    "            이를 정의하는 일은 드물지만 튜플이나 문자열과 같은 불변 유형을 \n",
    "            서브 클래싱하는 경우에는 그 용도가 있음.\n",
    "            자세한 내용은 아래 링크를 참고\n",
    "            https://www.python.org/download/releases/2.2/descrintro/#__new__\n",
    "        \"\"\"\n",
    "        replica.__dict__ = self.__dict__.copy()\n",
    "        replica._parameters = replica._parameters.copy()\n",
    "        replica._buffers = replica._buffers.copy()\n",
    "        replica._modules = replica._modules.copy()\n",
    "\n",
    "        # Warn users that gradients don't behave as expected on replica modules\n",
    "        old_zero_grad = replica.__class__.zero_grad\n",
    "        weak_self = weakref.ref(replica)\n",
    "\n",
    "        def zero_grad():\n",
    "            warnings.warn(\n",
    "                \"Calling .zero_grad() from a module that was passed to a nn.DataParallel() has no effect. \"\n",
    "                \"The parameters are copied (in a differentiable manner) from the original module. \"\n",
    "                \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n",
    "                \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n",
    "            replica = weak_self()\n",
    "            if replica:\n",
    "                old_zero_grad(replica)\n",
    "\n",
    "        replica.zero_grad = zero_grad\n",
    "\n",
    "        return replica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = nn.Module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Module()'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max Pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_lines = []\n",
    "extra_repr = net.extra_repr()\n",
    "extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(conv1):Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))',\n",
       " '(conv2):Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))',\n",
       " '(fc1):Linear(in_features=576, out_features=120, bias=True)',\n",
       " '(fc2):Linear(in_features=120, out_features=84, bias=True)',\n",
       " '(fc3):Linear(in_features=84, out_features=10, bias=True)']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, module in net._modules.items():\n",
    "    mod_str = repr(module)\n",
    "    mod_str = _addindent(mod_str, 4)\n",
    "    child_lines.append('(' + key + '):' + mod_str)\n",
    "child_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(conv1):Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))',\n",
       " '(conv2):Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))',\n",
       " '(fc1):Linear(in_features=576, out_features=120, bias=True)',\n",
       " '(fc2):Linear(in_features=120, out_features=84, bias=True)',\n",
       " '(fc3):Linear(in_features=84, out_features=10, bias=True)']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = extra_lines + child_lines\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Net('"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_str = net._get_name() + '('\n",
    "main_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lines:\n",
    "    if len(extra_lines) == 1 and not child_lines:\n",
    "        main_str += extra_lines[0]\n",
    "    else:\n",
    "        main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "        \n",
    "main_str += ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1):Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2):Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1):Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2):Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3):Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(main_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아니, local에서 상속받으면 왜 repr이 출력안되지?!\n",
    "## 밑히겠네...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
