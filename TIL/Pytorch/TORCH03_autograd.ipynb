{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH 03. Autograd: automotic differentiation\n",
    "- `autograd` package provides automatic differentiation for all operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor and set `requires_grad=True` to track computation with it\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "<AddBackward0 object at 0x000001C47BFBA160>\n",
      "\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do a tensor operation\n",
    "y = x + 2\n",
    "print(y, end='\\n\\n')\n",
    "\n",
    "# y was created as a result of an operation, so it has a `grad_fn`\n",
    "print(y.grad_fn, end='\\n\\n')\n",
    "\n",
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001C42E1803C8>\n"
     ]
    }
   ],
   "source": [
    "# `.requires_grad_( ... )` changes an existing Tensor's `requires_grad` flag in-place.\n",
    "# The input flag defaults to `False` if not given.\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)    # default is False\n",
    "a.requires_grad_(True)    # Set requres_grad as True\n",
    "print(a.requires_grad)    # It will be a True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)          # Since requires_grad is True, exists grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since `out` contains a single scalar,\n",
    "# `out.backward()` is equivalent to `out.backward(torch.tensor(1,))`.\n",
    "out.backward()  # 역전파 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Print gradients d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$o = {\\cfrac{1}{4}}{\\sum_{i}{z_{i}}}$$\n",
    "$$z_{i} = 3(x_{i}+2)^{2}$$\n",
    "$$z_{i}|_{x_{i}=1}=27$$\n",
    "$$\\text{Therefore,}{\\;}\\cfrac{{\\partial}o}{{\\partial}x_{i}}=\\cfrac{3}{2}(x_{i}+2)$$\n",
    "$${\\cfrac{{\\partial}o}{{\\partial}x_{i}}}\\bigg{|}_{x_{i}=1}=\\cfrac{9}{2}=4.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Mathematically, if you have a vector valued function}\\;\\vec{y}=f(\\vec{x}),$$\n",
    "$$\\text{then the gradient of}\\;\\vec{y}\\text{ with respect to}\\;\\vec{x}\\text{ is a jacobian matrix:}$$\n",
    "$$J=\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}y_{1}}{{\\partial}x_{1}} & \\cdots & \\cfrac{{\\partial}y_{1}}{{\\partial}x_{n}}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\cfrac{{\\partial}y_{m}}{{\\partial}x_{1}} & \\cdots & \\cfrac{{\\partial}y_{m}}{{\\partial}x_{n}}\\\\\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product.}$$\n",
    "$$\\text{That is, given any vector }v=(v1{\\quad}v2{\\quad}{\\cdots}{\\quad}v_{m})^{T}\\text{, compute the product }v^{T}{\\cdot}J$$\n",
    "$$\\text{If }v\\text{ happens to be the gradient of a scalar function }l=g\\big{(}\\vec{y}\\big{)}\\text{, that is, }v=\\bigg{(}\\cfrac{{\\partial}l}{{\\partial}y_{1}}\\;\\cdots\\;\\cfrac{{\\partial}l}{{\\partial}y_{m}}\\bigg{)}^{T}\\text{,}$$\n",
    "$$\\text{then by the chain rule, the vector-Jacobian product would be the gradient of }l\\text{ with respect to }\\vec{x}\\text{:}$$\n",
    "$$J^{T}\\cdot{v}=\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}y_{1}}{{\\partial}x_{1}} & \\cdots & \\cfrac{{\\partial}y_{m}}{{\\partial}x_{1}}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\cfrac{{\\partial}y_{1}}{{\\partial}x_{n}} & \\cdots & \\cfrac{{\\partial}y_{m}}{{\\partial}x_{n}}\\\\\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}l}{{\\partial}y_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\cfrac{{\\partial}l}{{\\partial}y_{m}}\\\\\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "\\cfrac{{\\partial}l}{{\\partial}x_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\cfrac{{\\partial}l}{{\\partial}x_{n}}\\\\\n",
    "\\end{pmatrix}$$\n",
    "$$\\text{(Note that }v^{T}\\cdot{J}\\text{ gives a row vector which can be treated as a column vector by taking }{J}^{T}\\cdot{v}\\text{)}$$\n",
    "$$\\text{This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : tensor([ 0.3164, -1.1508,  1.5400], requires_grad=True)\n",
      "y : tensor([  323.9894, -1178.4686,  1577.0007], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# vector-Jacobian product example\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x :', x)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print('y :', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "# y is not a scalar,\n",
    "# `torch.autograd` could not compute the full jacobian directly,\n",
    "# but if we just want the vector-jacobian product,\n",
    "# simply pass the vector to `backward` as argument.\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Stop autograd from tracking history on Tensors with `.requires_grad=True`\n",
    "# By wrapping the code block in `with torch.no_grad():`\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Or by using `.detach()` to get a new Tensor with the same content\n",
    "# but that does not require gradients\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE CODE FOR TORCH.AUTOGRAD\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.autograd.function import Function, NestedIOFunction\n",
    "from torch.autograd.gradcheck import gradcheck, gradgradcheck\n",
    "from torch.autograd.grad_mode import no_grad, enable_grad, set_grad_enabled\n",
    "from torch.autograd.anomaly_mode import detect_anomaly, set_detect_anomaly\n",
    "from torch.autograd import profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['Variable', 'Function', 'backward', 'grad_modea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_grads(outputs, grads):\n",
    "    new_grads = []\n",
    "    for out, grad in zip(outputs, grads):\n",
    "        if isinstance(grad, torch.Tensor):\n",
    "            if not out.shape == grad.shape:\n",
    "                raise RuntimeError(\"Mismatch in shape: grad_output[\"\n",
    "                                   + str(grads.index(grad)) + \"] has a shape of \"\n",
    "                                   + str(grad.shape) + \" and output[\"\n",
    "                                   + str(outputs.index(out)) + \"] has a shape of \"\n",
    "                                   + str(out.shape) + \".\")\n",
    "            new_grads.append(grad)\n",
    "        elif grad is None:\n",
    "            if out.requires_grad:\n",
    "                if out.numel() != 1:\n",
    "                    '''\n",
    "                    # Returns the total number of elements in the `input` tensor.\n",
    "                    >>> a = torch.randn(1, 2, 3, 4, 5)\n",
    "                    >>> torch.numel(a)\n",
    "                    120\n",
    "                    >>> a = torch.zeros(4, 4)\n",
    "                    >>> torch.numel(a)\n",
    "                    16\n",
    "                    '''\n",
    "                    raise RuntimeError(\"grad can be implicitly created only for scalar outputs\")\n",
    "                new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))\n",
    "            else:\n",
    "                new_grads.append(None)\n",
    "        else:\n",
    "            raise TypeError(\"gradients can be either Tensors or None, but got \" +\n",
    "                            type(grad).__name__)\n",
    "    return tuple(new_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False,\n",
    "             grad_variables=None):\n",
    "    \"\"\"\n",
    "    Computes the sum of gradients of given tensors w.r.t. graph leaves.\n",
    "    \"\"\"\n",
    "    if grad_variables is not None:\n",
    "        warnings.warn(\"'grad_variables' is deprecated. Use 'grad_tensors' instead.\")\n",
    "        if grad_tensors is None:\n",
    "            grad_tensors = grad_variables\n",
    "        else:\n",
    "            raise RuntimeErorr(\"'grad_tensors' and 'grad_variables' (deprecated) \"\n",
    "                               \"arguments both passed to backward(). Please only \"\n",
    "                               \"use 'grad_tensors'.\")\n",
    "    \n",
    "    tensors = (tensors, ) if isinstance(tensors, torch.Tensor) else tuple(tensors)\n",
    "    \n",
    "    if grad_tensors is None:\n",
    "        grad_tensors = [None] * len(tensors)\n",
    "    elif isinstance(grad_tensors, torch.Tensor):\n",
    "        grad_tensors = [grad_tensors]\n",
    "    else:\n",
    "        grad_tensors = list(grad_tensors)\n",
    "    \n",
    "    grad_tensors = _make_grads(tensors, grad_tensors)\n",
    "    if retain_graph is None:\n",
    "        retain_graph = create_graph\n",
    "        \n",
    "    # 위에서 설정만 잡아주고 돌리는건 C++ Imperative Engine에서 돌린다.\n",
    "    Variable._execution_engine.run_backward(\n",
    "        tensors, grad_tensors, retain_graph, create_graph,\n",
    "        allow_unreachable=True)  # allow_unreachable flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False,\n",
    "         only_inputs=True, allow_unused=False):\n",
    "    \"\"\"\n",
    "    Computes and returns the sum of gradients of outputs w.r.t. the inputs.\n",
    "    \"\"\"\n",
    "    if not only_inputs:\n",
    "        warnings.warn(\"only_inputs argument is deprecated and is ignored now \"\n",
    "                      \"(defualts to True). To accumulate gradient for other \"\n",
    "                      \"parts of the graph, please use torch.autograd.backward.\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
