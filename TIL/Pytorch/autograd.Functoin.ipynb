{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch._C as _C\n",
    "\n",
    "import functools\n",
    "import warnings\n",
    "from typing import Any\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import torch.utils.hooks as hooks\n",
    "# from torch.utils.hooks import RemovableHandle\n",
    "from collections import OrderedDict\n",
    "import weakref\n",
    "import warnings\n",
    "\n",
    "class RemovableHandle:\n",
    "    \n",
    "    next_id = 0\n",
    "    \n",
    "    def __init__(self, hooks_dict):\n",
    "        self.hooks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RemovableHandle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-58f3e737c67f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mhook\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m ) -> RemovableHandle:\n\u001b[0m\u001b[0;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m     12\u001b[0m     \u001b[0mpsuedo\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mregister\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RemovableHandle' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import Tensor, device, dtype\n",
    "from typing import Callable, Union, Tuple\n",
    "\n",
    "\n",
    "_grad_t = Union[Tuple[Tensor, ...], Tensor]\n",
    "\n",
    "def register_hook(\n",
    "    self,\n",
    "    hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]\n",
    ") -> RemovableHandle:\n",
    "    \"\"\"\n",
    "    psuedo code for Module's register hook_fn\n",
    "    _hooks는 아래 세 가지 중 하나\n",
    "        ``_forward_pre_hooks``\n",
    "        ``_forward_hooks``\n",
    "        ``_backward_hooks``\n",
    "    attr: ``hook``은 아래와 같은 함수\n",
    "        # 아래 함수를 정의하여 gradient를 쉽게 다룰 수 있음\n",
    "        # 코드가 돌아가며 gradient가 어떻게 변화하는지 디버깅하거나\n",
    "        # 중간에 결과값을 반환하여 다른 작업 등을 할 수 있음\n",
    "        def hook_fn(module, grad_input, grad_output):\n",
    "            print(grad_input)\n",
    "            print(grad_output)\n",
    "            return (grad_input, grad_output)\n",
    "    hook 등록 메서드가 작동하는 방식은 단순히 추가하는 방식\n",
    "    \"\"\"\n",
    "    handle = RemovableHandle(self._hooks)\n",
    "    self.hooks[handle.id] = hook\n",
    "    return handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_methclass(meta, *bases):\n",
    "    \"\"\"Create a base class with a metaclass\"\"\"\n",
    "    class metaclass(meta):\n",
    "        def __new__(cls, name, this_bases, d):\n",
    "            return meta(name, bases, d)\n",
    "    return type.__new__(metaclass, 'temporary_class', (), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__new__', 'apply', '_do_backward', '_register_hook_dict', 'register_hook', 'saved_tensors', 'saved_variables', 'next_functions', 'to_save', 'non_differentiable', 'dirty_tensors', 'needs_input_grad', 'requires_grad', 'metadata', '__doc__'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_C._FunctionBase.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ContextMethodMixin:\n",
    "    \n",
    "    def save_for_backward(self, *tensors):\n",
    "        \"\"\"\n",
    "        :func:`~Function.backward`를 위해 주어진 tensor를 저장\n",
    "        \n",
    "        이 메서드는 :func:`forward` 메서드 내부에서 한 번만 호출돼야 함\n",
    "        \n",
    "        이후 저장된 tensor는 :attr:`saved_tensors` 속성으로 접근 가능\n",
    "        위 tensor가 user에게 반환되기 전, in-place 연산이 있나 check\n",
    "        인자는 ``None``이 되도 상관 없음.        \n",
    "        \"\"\"\n",
    "        self.to_save = tensors\n",
    "        \n",
    "    def make_dirty(self, *args):\n",
    "        \"\"\"\n",
    "        in-place 연산이 수행된 tensor를 mark\n",
    "        \n",
    "        이 메서드는 :func:`forward` 메서드 내부에서 한 번만 호출돼야 함\n",
    "        그리고 모든 인자는 inputs이 돼야 함\n",
    "        \n",
    "        :func:`forward` 호출에서 수정된 모든 tensor는 \n",
    "        check 연산의 정확성을 보증하기 위해 이 함수에 주어짐\n",
    "        함수가 호출 되기 전, 된 후에 수정됐는지 여부는 관계 X        \n",
    "        \"\"\"\n",
    "        self.dirty_tensors = args\n",
    "        \n",
    "    def mark_shared_storage(self, *pairs):\n",
    "        warnings.warn(\n",
    "            'mark_shared_storage is deprecated. '\n",
    "            'Tensors with shared storages are automatically tracked. Note '\n",
    "            'that calls to `set_()` are not tracked')\n",
    "    \n",
    "    def mark_non_differentiable(self, *args):\n",
    "        \"\"\"\n",
    "        output들을 non-differentiable로 표시\n",
    "        \n",
    "        이 메서드는 :func:`forward` 메서드 내부에서 한 번만 호출돼야 함\n",
    "        그리고 모든 인자는 outputs이 돼야 함\n",
    "        \n",
    "        outputs에 gradient가 필요하지 않다고 표시함 (backward 연산의 효율성을 위해서!)\n",
    "        :meth:`~Function.backward`의 각 출력에 gradient를 허용해야 하지만,\n",
    "        이는 항상 출력의 shape과 동일한 shape의 zero tensor가 됨\n",
    "        \n",
    "        This is used e.g. for indices returned from a max :class:`Function`.(해석불가)\n",
    "        \"\"\"\n",
    "        self.non_differentiable = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _HookMixin:\n",
    "    \n",
    "    \"\"\"\n",
    "    `torch.nn.Module`에도 있었던 method\n",
    "    만일 backward_hooks이 없으면 OrderedDict로 handler\n",
    "    아래 메서드는 일전에 본 객체와 동일\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _register_hook(backward_hooks, hook):\n",
    "        if backward_hooks is None:\n",
    "            backward_hooks = OrderedDict()\n",
    "#         handle = hooks.RemovableHandle(backward_hooks)\n",
    "        handle = RemovableHandle(backward_hooks)\n",
    "        backward_hooks[handle.id] = hook\n",
    "        return backward_hooks, handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardCFunction(\n",
    "    _C._FunctionBase,\n",
    "    _ContextMethodMixin,\n",
    "    \n",
    "):\n",
    "\n",
    "\n",
    "class FunctionMeta(type):\n",
    "    \n",
    "    def __init__(cls, name, bases, attrs):\n",
    "        for super_cls in cls.mro():\n",
    "            forward = super_cls.__dict__.get(\"forward\")\n",
    "            if forward is not None:\n",
    "                has_static_forward = isinstance(forward, staticmethod)\n",
    "                break\n",
    "        cls._is_legacy = not has_static_forward\n",
    "        \n",
    "        # old-style functions\n",
    "        # forward 메서드가 staticmethod로 정의\n",
    "        if not has_static_forward:\n",
    "            return super(FunctionMeta, cls).__init__(name, bases, attrs)\n",
    "        \n",
    "        backward_fn = type(\n",
    "            name + 'Backward',     # name\n",
    "            (BackwardCFunction,),  # bases\n",
    "            {'_forward_cls': cls}  # attrs\n",
    "        )\n",
    "        cls._backward_cls = backward_fn\n",
    "        \n",
    "        return super(FunctionMeta, cls).__init__(name, bases, attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function():\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise RuntimeError(\n",
    "            \"Legacy autograd function with non-static forward method is deprecated. \"\n",
    "            \"Please use new-style autograd function with static forward method. \"\n",
    "            \"(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\")\n",
    "    \n",
    "    # for the tensor\n",
    "    is_traceable = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:\n",
    "        raise NotImplementedError(\n",
    "            \"You must implement the forward function for custom\"\n",
    "            \" autograd.Function.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, *grad_output: Any) -> Any:\n",
    "        raise NotImplementedError(\n",
    "            \"You must implement the backward function for custom\"\n",
    "            \" autograd.Function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_metaclass(FunctionMeta, _C._FunctionBase, _ContextMethodMixin, _HookMixin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
