### 2020-11-12 **[Sharp Minima Can Generalize For Deep Nets](https://arxiv.org/abs/1703.04933)**
> Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio
>
> DeepMind and Montreal

- SGD가 Batch method보다 잘되는 이유를 Loss Local Minima의 Flat vs Sharp (Wide vs Narrow)로 설명하곤했다.
- 본 논문에선 Neural Network (특히 Rectified)의 구조 특성 상, minima의 flat 혹은 sharp한 여부가 일반화와 무관하다는 것을 4가지의 수학적 증명을 통해 밝혀냈다.-
- 즉, batch size와 generalization 사이에 인과관계는 없다.

